
#Title: "Predict the manner in which exercises were performed"

Author: "Pnagaraj"
Date: "November 16, 2015"
Output: html_document

## Executive summary

The objective of this report is to predict the quality of a certain exercise activity from measurements. In this study, participants were asked to perform dumbbell lifts. The way in which the exercise was done by the participants (in one correct way and 4 incorrect ways) were measured by sensors attached to the belt, the forearm, the arm and the dumbbell. For analysis, the training data was provided which was cleaned and divided into training and cross-validation sets. Further, two models were tested. One of the models had features preprocessed by using principle component analysis prior to applying the random forest algorithm. The second model consisted of running random tree algorithm without principle component analysis. The out of sample error was compared for the two models by applying the models to cross validation data. It was found that the model with PCA had a higher out of sample error compared to the model without. Finally, the model with lower out of sample error was chosen to predict the outcome on test data. 

## Exploratory data analysis

```{r loadlib, echo=TRUE}
library(ggplot2)
library(caret)
library(AppliedPredictiveModeling)
library(randomForest)

```

The training and test data were downloaded from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv, respectively. 

```{r loaddata, echo=TRUE}
# read data and identify missing or incorrect or wrong values as NA
dat = read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!",""))
# remove columesn that are NA for all rows, these columns have NA for majority of the rows in this data.
training=dat[,colSums(is.na(dat))==0]
#training$classe=is.numeric(training$classe)
# Perform the same cleaning on test data
dat = read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!",""))
testing=dat[,colSums(is.na(dat))==0]
training=subset(training,select=c(-X,-user_name,-raw_timestamp_part_1,-raw_timestamp_part_2,-cvtd_timestamp,-new_window,-num_window))
testing=subset(testing,select=c(-X,-user_name,-raw_timestamp_part_1,-raw_timestamp_part_2,-cvtd_timestamp,-new_window,-num_window))
```

Training and test data were cleaned by removing predictors that do not have valid values namely, "NA", "#DIV/0!" and no value in their entirety. Next, the non-numeric variables containing user id, user-name, timestamp and window information were removed from the analysis. Timestamp and window information was considered irrelevant for the current analysis assuming that the quality of the activities did not depend on time. After data cleaning, there were 52 predictors for the classe variable. The number of observations available in the training data was now `r format(nrow(training),scientific=FALSE)` and there were `r format(nrow(testing),scientific=FALSE)` observations in the testing dataset.

## Prediction model bulding

The training data was partitioned into training (70%) and cross validation (30%) datasets. 

Two types of model building were compared, one (a) with PCA (Principle component analysis) and another (b) without PCA. PCA removes highly correlated predictors from the training method. This helps in balancing the tradeoff between bias and variance. In particular, PCA can help avoid overfitting the training data. However, if the data measurements are such that the predictors are not correlated or are weakly correlated, then the variance can increase. Therefore, the two models as in (a) and (b) were compared. 

In (a) the model with PCA, the training data was centred and scaled prior to applying PCA.  PCA was applied to remove highly correlated predictors. Variables with 90% of the variance were retained. This reduced the number of predictors to 18 from 52. New data values were generated from the predictors provided by PCA. Random forest algorithm with 200 trees using cross-validation technique was applied to train the resulting data. Finally, the trained model was tested on the cross validation data. Confusion matrix was generated between the training and the cross validation data and the accuracy of the method was measured. 

In (b) the model without PCA, all the predictors in the initial training set were used to train the model using random forest algorithm. As before, 200 trees using cross-validation technique was used to train the model. The trained model was tested on the cross validation data. Confusion matrix was generated between the training and the cross validation data and the accuracy of the method was measured. 
Accuracy gives a measure of the out-of-sample error as it is measured on the cross-validation data.

```{r model, echo=FALSE}
set.seed(3433)
idx=createDataPartition(training$classe,p=.7)[[1]]
# training data to test PCA
traindat=training[idx,]
trclass = traindat$classe
# crossvalidation data
cvdat=training[-idx,]
cvclass = cvdat$classe
# perform PCA on training and evaluate on cv data

preproc = preProcess(traindat[,-53],method=c("center","scale")) # center and scale the data before PCA
traindatsc=predict(preproc,traindat[,-53]) 
# now apply PCA, keeping 90% of the variance
pobj = preProcess(traindatsc,method="pca",thresh=0.90)
sp2 = predict(pobj,traindat[,-53]) 
traindatpca=sp2
traindatpca$classe <- traindat$classe 
m2 = train(classe~.,method="rf",data=traindatpca,trControl = trainControl(method="cv"),ntree=200)
p3 = predict(pobj,cvdat[,-53])
accpc=confusionMatrix(cvdat$classe,predict(m2,p3))

## now train without PCA
m2a = train(classe~.,method="rf",data=traindat,trControl = trainControl(method="cv"),ntree=200)
p3a = predict(m2a,cvdat[,-53])
acc=confusionMatrix(cvdat$classe,p3a)

```

## Analysis
For model (a), the confusion matrix is shown below:

```{r analpc, echo=FALSE}
accpc$table
```

The confusion matrix for model (b) is shown below:
```{r anal, echo=FALSE}
acc$table
```

The confusion matrix for model (b) shows more true-positives compared to model (a). The accuracy measure captures this information where the accuracy of model (a) was `r format(accpc$overall[[1]],SCIENTIFIC=FALSE)` and that of model (b) was `r format(acc$overall[[1]],SCIENTIFIC=FALSE)` . Model (b) had higher accuracy than model (a) suggesting that the out-of-sample error is lower in model (b). Further, this measure also seems to indicate that the predictors were likely independent and thus PCA was not needed to reduce the number of predictors. 

## Predict on Test data

Next, model (b) was applied to the original test data and the outcome on the test data was displayed. 

```{r test, echo=TRUE}
testoutcome = predict(m2a,testing)
answers = testoutcome
# script to write answers to files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)

```